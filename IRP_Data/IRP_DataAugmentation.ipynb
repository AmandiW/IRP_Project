{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm  # Using notebook version for better display in Jupyter\n",
    "import random\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:31:10.442989700Z",
     "start_time": "2025-04-06T17:31:10.308231200Z"
    }
   },
   "id": "9ed714b9ec09dc25"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# Define paths\n",
    "ORIGINAL_IMAGES_PATH = r\"E:\\IRP_dataset_new\\APTOS_images\"\n",
    "ORIGINAL_LABELS_PATH = r\"E:\\IRP_dataset_new\\APTOS_labels_cleaned.csv\"\n",
    "AUGMENTED_IMAGES_PATH = r\"E:\\IRP_dataset_new\\augmented_images\"\n",
    "AUGMENTED_LABELS_PATH = r\"E:\\IRP_dataset_new\\augmented_labels.csv\"\n",
    "COMBINED_IMAGES_PATH = r\"E:\\IRP_dataset_new\\APTOS_combined_images\"\n",
    "COMBINED_LABELS_PATH = r\"E:\\IRP_dataset_new\\APTOS_labels_combined.csv\"\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(AUGMENTED_IMAGES_PATH, exist_ok=True)\n",
    "os.makedirs(COMBINED_IMAGES_PATH, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:31:10.442989700Z",
     "start_time": "2025-04-06T17:31:10.323984100Z"
    }
   },
   "id": "c8971bf817d01b96"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset contains 3653 entries\n",
      "Columns in the CSV: ['id_code', 'diagnosis']\n",
      "Class distribution: \n",
      "diagnosis\n",
      "1    1853\n",
      "0    1800\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the original labels\n",
    "df_original = pd.read_csv(ORIGINAL_LABELS_PATH)\n",
    "print(f\"Original dataset contains {len(df_original)} entries\")\n",
    "print(f\"Columns in the CSV: {df_original.columns.tolist()}\")\n",
    "print(f\"Class distribution: \\n{df_original['diagnosis'].value_counts()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:31:10.442989700Z",
     "start_time": "2025-04-06T17:31:10.342212800Z"
    }
   },
   "id": "e2e6d463a0707a72"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring image directory...\n",
      "Found 3653 image files in E:\\IRP_dataset_new\\APTOS_images\n"
     ]
    }
   ],
   "source": [
    "# Explore the image directory\n",
    "print(\"\\nExploring image directory...\")\n",
    "extensions = ['.png', '.jpg', '.jpeg', '.tif', '.tiff']  # Put .png first based on the user's example\n",
    "available_images = []\n",
    "for ext in extensions:\n",
    "    files = glob.glob(os.path.join(ORIGINAL_IMAGES_PATH, f\"*{ext}\"))\n",
    "    available_images.extend(files)\n",
    "\n",
    "print(f\"Found {len(available_images)} image files in {ORIGINAL_IMAGES_PATH}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:31:11.276862100Z",
     "start_time": "2025-04-06T17:31:10.415799200Z"
    }
   },
   "id": "2f3c73d1d6c46026"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image filenames:\n",
      "  000c1434d8d7.png\n",
      "  001639a390f0.png\n",
      "  0024cdab0c1e.png\n",
      "  002c21358ce6.png\n",
      "  005b95c28852.png\n"
     ]
    }
   ],
   "source": [
    "# Show some sample filenames\n",
    "if available_images:\n",
    "    print(\"Sample image filenames:\")\n",
    "    for img_path in available_images[:5]:\n",
    "        print(f\"  {os.path.basename(img_path)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:31:11.339131500Z",
     "start_time": "2025-04-06T17:31:11.276862100Z"
    }
   },
   "id": "97f61ca4c3755988"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# Function to find the file by ID (trying different extensions)\n",
    "def find_image_file(id_code, directory):\n",
    "    \"\"\"Find the image file with the given ID in the directory, trying different extensions\"\"\"\n",
    "    for ext in extensions:\n",
    "        file_path = os.path.join(directory, f\"{id_code}{ext}\")\n",
    "        if os.path.exists(file_path):\n",
    "            return file_path, ext\n",
    "    return None, None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:31:11.355007900Z",
     "start_time": "2025-04-06T17:31:11.291874300Z"
    }
   },
   "id": "5ea5f5ef3004bd8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to perform simple image augmentation using OpenCV\n",
    "def augment_image(image, augmentation_type):\n",
    "    \"\"\"\n",
    "    Perform image augmentation using OpenCV\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image to augment\n",
    "    - augmentation_type: Integer specifying the type of augmentation\n",
    "      1: Rotation + horizontal flip\n",
    "      2: Translation + brightness + slight zoom\n",
    "    \n",
    "    Returns:\n",
    "    - Augmented image\n",
    "    \"\"\"\n",
    "    result = image.copy()\n",
    "    \n",
    "    if augmentation_type == 1:  # Rotation + horizontal flip\n",
    "        # Rotation (more randomized angle)\n",
    "        angle = random.uniform(-25, 25)\n",
    "        height, width = result.shape[:2]\n",
    "        center = (width/2, height/2)\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        result = cv2.warpAffine(result, rotation_matrix, (width, height), \n",
    "                              flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "        \n",
    "        # Horizontal flip (preserves anatomical correctness)\n",
    "        if random.random() > 0.5:\n",
    "            result = cv2.flip(result, 1)\n",
    "            \n",
    "        # Minor brightness adjustment for added uniqueness\n",
    "        brightness = random.uniform(0.9, 1.1)\n",
    "        result = cv2.convertScaleAbs(result, alpha=brightness, beta=0)\n",
    "    \n",
    "    elif augmentation_type == 2:  # Translation + brightness + slight zoom\n",
    "        # Translation (more randomized shift)\n",
    "        height, width = result.shape[:2]\n",
    "        tx = random.uniform(-0.15, 0.15) * width\n",
    "        ty = random.uniform(-0.15, 0.15) * height\n",
    "        translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        result = cv2.warpAffine(result, translation_matrix, (width, height), \n",
    "                              borderMode=cv2.BORDER_REFLECT)\n",
    "        \n",
    "        # Brightness adjustment (more randomized)\n",
    "        brightness = random.uniform(0.7, 1.3)\n",
    "        result = cv2.convertScaleAbs(result, alpha=brightness, beta=0)\n",
    "        \n",
    "        # Add slight zoom effect for uniqueness\n",
    "        zoom_factor = random.uniform(0.9, 1.1)\n",
    "        height, width = result.shape[:2]\n",
    "        center_x, center_y = width // 2, height // 2\n",
    "        new_width, new_height = int(width * zoom_factor), int(height * zoom_factor)\n",
    "        \n",
    "        # Calculate crop coordinates\n",
    "        x1 = max(0, center_x - new_width // 2)\n",
    "        y1 = max(0, center_y - new_height // 2)\n",
    "        x2 = min(width, center_x + new_width // 2)\n",
    "        y2 = min(height, center_y + new_height // 2)\n",
    "        \n",
    "        # Crop and resize\n",
    "        cropped = result[y1:y2, x1:x2]\n",
    "        result = cv2.resize(cropped, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Generate augmented images\n",
    "augmented_rows = []\n",
    "print(\"\\nGenerating augmented images...\")\n",
    "\n",
    "# Check extension of first few images to determine the most common extension\n",
    "extension_counts = {}\n",
    "for img_path in available_images[:20]:\n",
    "    ext = os.path.splitext(img_path)[1].lower()\n",
    "    extension_counts[ext] = extension_counts.get(ext, 0) + 1\n",
    "\n",
    "# Use the most common extension for saving augmented images\n",
    "if extension_counts:\n",
    "    most_common_ext = max(extension_counts.items(), key=lambda x: x[1])[0]\n",
    "    print(f\"Most common extension is {most_common_ext}, using this for augmented images\")\n",
    "else:\n",
    "    most_common_ext = '.png'  # Default to png based on the user's example\n",
    "    print(f\"Using default extension {most_common_ext} for augmented images\")\n",
    "\n",
    "for index, row in tqdm(df_original.iterrows(), total=len(df_original)):\n",
    "    id_code = row['id_code']\n",
    "    diagnosis = row['diagnosis']\n",
    "    \n",
    "    # Find the image file\n",
    "    original_img_path, detected_ext = find_image_file(id_code, ORIGINAL_IMAGES_PATH)\n",
    "    \n",
    "    if original_img_path is None:\n",
    "        print(f\"Warning: Original image not found for {id_code}\")\n",
    "        continue\n",
    "    \n",
    "    # Read the image\n",
    "    img = cv2.imread(original_img_path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: Could not read image for {id_code} at {original_img_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Generate 2 distinctly different augmented versions\n",
    "    for aug_num in range(1, 3):\n",
    "        # Create new ID for the augmented image\n",
    "        new_id_code = f\"{id_code}_aug_{aug_num}\"\n",
    "        new_img_path = os.path.join(AUGMENTED_IMAGES_PATH, f\"{new_id_code}{most_common_ext}\")\n",
    "        \n",
    "        # Apply augmentation (use aug_num to determine augmentation type)\n",
    "        aug_img = augment_image(img, aug_num)\n",
    "        \n",
    "        # Save the augmented image\n",
    "        if cv2.imwrite(new_img_path, aug_img):\n",
    "            # Add to augmented labels\n",
    "            augmented_rows.append({\n",
    "                'id_code': new_id_code,\n",
    "                'diagnosis': diagnosis\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Failed to save augmented image for {id_code} at {new_img_path}\")\n",
    "\n",
    "# Create DataFrame for augmented labels\n",
    "df_augmented = pd.DataFrame(augmented_rows)\n",
    "print(f\"\\nGenerated {len(df_augmented)} augmented images\")\n",
    "if len(df_augmented) > 0:\n",
    "    print(f\"Augmented class distribution: \\n{df_augmented['diagnosis'].value_counts()}\")\n",
    "    \n",
    "    # Save augmented labels\n",
    "    df_augmented.to_csv(AUGMENTED_LABELS_PATH, index=False)\n",
    "    print(f\"Saved augmented labels to {AUGMENTED_LABELS_PATH}\")\n",
    "else:\n",
    "    print(\"WARNING: No augmented images were created!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da8b0c7377f85af0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine original and augmented datasets\n",
    "# 1. Copy original images to combined directory\n",
    "print(\"\\nCopying original images to combined directory...\")\n",
    "for index, row in tqdm(df_original.iterrows(), total=len(df_original)):\n",
    "    id_code = row['id_code']\n",
    "    original_img_path, detected_ext = find_image_file(id_code, ORIGINAL_IMAGES_PATH)\n",
    "    \n",
    "    if original_img_path is not None:\n",
    "        dst = os.path.join(COMBINED_IMAGES_PATH, f\"{id_code}{detected_ext}\")\n",
    "        shutil.copy2(original_img_path, dst)\n",
    "\n",
    "# 2. Copy augmented images to combined directory\n",
    "print(\"Copying augmented images to combined directory...\")\n",
    "for index, row in tqdm(df_augmented.iterrows(), total=len(df_augmented)):\n",
    "    id_code = row['id_code']\n",
    "    src = os.path.join(AUGMENTED_IMAGES_PATH, f\"{id_code}{most_common_ext}\")\n",
    "    if os.path.exists(src):\n",
    "        dst = os.path.join(COMBINED_IMAGES_PATH, f\"{id_code}{most_common_ext}\")\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "# 3. Combine labels\n",
    "df_combined = pd.concat([df_original, df_augmented], ignore_index=True)\n",
    "print(f\"\\nCombined dataset contains {len(df_combined)} entries\")\n",
    "print(f\"Combined class distribution: \\n{df_combined['diagnosis'].value_counts()}\")\n",
    "\n",
    "# Calculate the expansion factor\n",
    "if len(df_original) > 0:\n",
    "    expansion_factor = len(df_combined) / len(df_original)\n",
    "    print(f\"Dataset has been expanded by a factor of {expansion_factor:.2f}x\")\n",
    "    \n",
    "    # Verify we have the expected 3x expansion\n",
    "    if abs(expansion_factor - 3.0) < 0.1:\n",
    "        print(\"Successfully created a dataset 3x the original size!\")\n",
    "    else:\n",
    "        print(f\"WARNING: Dataset expansion factor ({expansion_factor:.2f}x) differs from the expected 3x\")\n",
    "\n",
    "# Save combined labels\n",
    "df_combined.to_csv(COMBINED_LABELS_PATH, index=False)\n",
    "print(f\"Saved combined labels to {COMBINED_LABELS_PATH}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-06T17:32:35.533655800Z"
    }
   },
   "id": "54812f32f9ee09b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verify data consistency\n",
    "print(\"\\nVerifying data consistency...\")\n",
    "image_files = set()\n",
    "for ext in extensions:\n",
    "    files = glob.glob(os.path.join(COMBINED_IMAGES_PATH, f\"*{ext}\"))\n",
    "    image_files.update([os.path.splitext(os.path.basename(f))[0] for f in files])\n",
    "\n",
    "label_ids = set(df_combined['id_code'])\n",
    "\n",
    "# Find inconsistencies\n",
    "missing_images = label_ids - image_files\n",
    "orphaned_images = image_files - label_ids\n",
    "\n",
    "print(f\"Found {len(missing_images)} labels without corresponding images\")\n",
    "print(f\"Found {len(orphaned_images)} images without corresponding labels\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-06T17:32:35.538447200Z"
    }
   },
   "id": "881577dceadebe7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clean up the dataset if necessary\n",
    "if missing_images or orphaned_images:\n",
    "    print(\"Cleaning up inconsistencies...\")\n",
    "    \n",
    "    # Remove labels without images\n",
    "    if missing_images:\n",
    "        df_combined = df_combined[~df_combined['id_code'].isin(missing_images)]\n",
    "    \n",
    "    # Remove images without labels\n",
    "    if orphaned_images:\n",
    "        for img_id in orphaned_images:\n",
    "            # Try to find and remove the image file with any extension\n",
    "            found = False\n",
    "            for ext in extensions:\n",
    "                img_path = os.path.join(COMBINED_IMAGES_PATH, f\"{img_id}{ext}\")\n",
    "                if os.path.exists(img_path):\n",
    "                    os.remove(img_path)\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            if not found:\n",
    "                print(f\"Warning: Could not find image file for orphaned ID {img_id}\")\n",
    "    \n",
    "    # Save the cleaned dataset\n",
    "    df_combined.to_csv(COMBINED_LABELS_PATH, index=False)\n",
    "    print(f\"Saved cleaned combined labels to {COMBINED_LABELS_PATH}\")\n",
    "\n",
    "print(f\"\\nFinal dataset contains {len(df_combined)} entries\")\n",
    "print(f\"Final class distribution: \\n{df_combined['diagnosis'].value_counts()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:32:35.545960100Z",
     "start_time": "2025-04-06T17:32:35.545960100Z"
    }
   },
   "id": "9c1ffc37344af5cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display 5 samples from each class\n",
    "def display_samples(samples, title):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (index, row) in enumerate(samples.iterrows(), 1):\n",
    "        id_code = row['id_code']\n",
    "        \n",
    "        # Try to find the image with any extension\n",
    "        img_path = None\n",
    "        for ext in extensions:\n",
    "            temp_path = os.path.join(COMBINED_IMAGES_PATH, f\"{id_code}{ext}\")\n",
    "            if os.path.exists(temp_path):\n",
    "                img_path = temp_path\n",
    "                break\n",
    "        \n",
    "        if img_path is not None:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Convert BGR to RGB for proper display\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                plt.subplot(1, 5, i)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"ID: {id_code}\\nClass: {row['diagnosis']}\")\n",
    "                plt.axis('off')\n",
    "            else:\n",
    "                print(f\"Warning: Could not read image {img_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Image not found for {id_code}\")\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-06T17:32:35.545960100Z"
    }
   },
   "id": "70e473e5bd31b286"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from collections import defaultdict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-06T17:32:35.545960100Z"
    }
   },
   "id": "371e40bc7a0e37ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define paths\n",
    "COMBINED_IMAGES_PATH = r\"E:\\IRP_dataset_new\\APTOS_combined_images\"\n",
    "COMBINED_LABELS_PATH = r\"E:\\IRP_dataset_new\\APTOS_labels_combined.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-06T17:32:35.560976400Z"
    }
   },
   "id": "2942d9b31c414953"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extensions to check\n",
    "extensions = ['.png', '.jpg', '.jpeg', '.tif', '.tiff']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:32:35.608535900Z",
     "start_time": "2025-04-06T17:32:35.568811400Z"
    }
   },
   "id": "d229c7e3c59d88d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to compute the hash of an image file\n",
    "def get_file_hash(file_path):\n",
    "    \"\"\"Calculate SHA-256 hash of a file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "        return file_hash\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating hash for {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to verify image can be opened and is valid\n",
    "def verify_image(file_path):\n",
    "    \"\"\"Check if an image file can be opened and is valid\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(file_path)\n",
    "        if img is None:\n",
    "            return False, \"Cannot be read by OpenCV\"\n",
    "        \n",
    "        # Check for completely black or white images\n",
    "        if img.size == 0:\n",
    "            return False, \"Empty image\"\n",
    "        \n",
    "        # Check if image is all black or all white\n",
    "        if np.mean(img) < 1 or np.mean(img) > 254:\n",
    "            return False, \"Image is all black or all white\"\n",
    "        \n",
    "        # Additional check: make sure image is not too small\n",
    "        h, w = img.shape[:2]\n",
    "        if h < 10 or w < 10:\n",
    "            return False, f\"Image too small ({w}x{h})\"\n",
    "            \n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(\"\\n1. Reading labels file...\")\n",
    "try:\n",
    "    df_labels = pd.read_csv(COMBINED_LABELS_PATH)\n",
    "    print(f\"Found {len(df_labels)} entries in the labels file\")\n",
    "    print(f\"Columns: {df_labels.columns.tolist()}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    if 'id_code' not in df_labels.columns:\n",
    "        print(\"ERROR: 'id_code' column not found in labels file\")\n",
    "        id_column = df_labels.columns[0]  # Assume first column is ID\n",
    "        print(f\"Using '{id_column}' as ID column\")\n",
    "    else:\n",
    "        id_column = 'id_code'\n",
    "        \n",
    "    if 'diagnosis' not in df_labels.columns:\n",
    "        print(\"ERROR: 'diagnosis' column not found in labels file\")\n",
    "        \n",
    "    # Check for duplicate IDs in labels\n",
    "    duplicate_ids = df_labels[df_labels.duplicated(subset=[id_column], keep=False)]\n",
    "    if len(duplicate_ids) > 0:\n",
    "        print(f\"WARNING: Found {len(duplicate_ids)} duplicate IDs in labels file\")\n",
    "        print(\"First few duplicates:\")\n",
    "        print(duplicate_ids.head())\n",
    "    else:\n",
    "        print(\"No duplicate IDs found in labels file\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ERROR reading labels file: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n2. Scanning image directory...\")\n",
    "# Get all image files\n",
    "all_image_files = []\n",
    "for ext in extensions:\n",
    "    files = glob.glob(os.path.join(COMBINED_IMAGES_PATH, f\"*{ext}\"))\n",
    "    all_image_files.extend(files)\n",
    "\n",
    "print(f\"Found {len(all_image_files)} image files\")\n",
    "\n",
    "# Extract IDs from filenames\n",
    "image_ids = []\n",
    "for img_path in all_image_files:\n",
    "    img_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    image_ids.append((img_id, img_path))\n",
    "\n",
    "# Check if all image IDs from labels exist in the directory\n",
    "print(\"\\n3. Checking for missing images...\")\n",
    "label_ids = set(df_labels[id_column])\n",
    "found_image_ids = {img_id for img_id, _ in image_ids}\n",
    "\n",
    "missing_images = label_ids - found_image_ids\n",
    "if missing_images:\n",
    "    print(f\"WARNING: Found {len(missing_images)} IDs in labels without corresponding images\")\n",
    "    print(\"First few missing IDs:\")\n",
    "    print(list(missing_images)[:5])\n",
    "else:\n",
    "    print(\"No missing images found\")\n",
    "\n",
    "# Check if all image files have corresponding labels\n",
    "orphaned_images = found_image_ids - label_ids\n",
    "if orphaned_images:\n",
    "    print(f\"WARNING: Found {len(orphaned_images)} images without corresponding labels\")\n",
    "    print(\"First few orphaned images:\")\n",
    "    print(list(orphaned_images)[:5])\n",
    "else:\n",
    "    print(\"No orphaned images found\")\n",
    "    \n",
    "# Check for invalid or corrupted images\n",
    "print(\"\\n4. Checking for invalid or corrupted images...\")\n",
    "invalid_images = []\n",
    "\n",
    "print(\"Processing images to check validity...\")\n",
    "for img_id, img_path in tqdm(image_ids):\n",
    "    is_valid, error_msg = verify_image(img_path)\n",
    "    if not is_valid:\n",
    "        invalid_images.append((img_id, img_path, error_msg))\n",
    "\n",
    "if invalid_images:\n",
    "    print(f\"WARNING: Found {len(invalid_images)} invalid or corrupted images\")\n",
    "    print(\"First few invalid images:\")\n",
    "    for img_id, img_path, error_msg in invalid_images[:5]:\n",
    "        print(f\"  {img_id}: {error_msg}\")\n",
    "else:\n",
    "    print(\"No invalid images found\")\n",
    "\n",
    "# Check for duplicate images (same content but different filenames)\n",
    "print(\"\\n5. Checking for duplicate image content...\")\n",
    "print(\"Computing image hashes (this may take some time)...\")\n",
    "\n",
    "# Compute hashes for all files\n",
    "file_hashes = {}\n",
    "hash_to_files = defaultdict(list)\n",
    "\n",
    "for img_id, img_path in tqdm(image_ids):\n",
    "    file_hash = get_file_hash(img_path)\n",
    "    if file_hash:\n",
    "        file_hashes[img_path] = file_hash\n",
    "        hash_to_files[file_hash].append((img_id, img_path))\n",
    "\n",
    "# Find duplicates (files with the same hash)\n",
    "duplicates = {h: files for h, files in hash_to_files.items() if len(files) > 1}\n",
    "\n",
    "if duplicates:\n",
    "    print(f\"WARNING: Found {len(duplicates)} sets of duplicate images (same content, different names)\")\n",
    "    print(\"First few duplicate sets:\")\n",
    "    for i, (file_hash, files) in enumerate(list(duplicates.items())[:3]):\n",
    "        print(f\"Duplicate set {i+1}:\")\n",
    "        for img_id, img_path in files:\n",
    "            print(f\"  {img_id}: {img_path}\")\n",
    "else:\n",
    "    print(\"No duplicate image content found\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-06T17:32:35.575951Z"
    }
   },
   "id": "cb13872b4ddfc534"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Identifies and removes duplicate images while preserving dataset integrity\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "COMBINED_IMAGES_PATH = r\"E:\\IRP_dataset_new\\APTOS_combined_images\"\n",
    "COMBINED_LABELS_PATH = r\"E:\\IRP_dataset_new\\APTOS_labels_combined.csv\"\n",
    "BACKUP_DIR = r\"E:\\IRP_dataset_new\\duplicates_backup\"  # Directory to back up removed files\n",
    "\n",
    "# Extensions to check\n",
    "extensions = ['.png', '.jpg', '.jpeg', '.tif', '.tiff']\n",
    "\n",
    "print(\"APTOS Dataset Duplicate Removal\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create backup directory if it doesn't exist\n",
    "os.makedirs(BACKUP_DIR, exist_ok=True)\n",
    "print(f\"Created backup directory at {BACKUP_DIR}\")\n",
    "\n",
    "# Function to compute the hash of an image file\n",
    "def get_file_hash(file_path):\n",
    "    \"\"\"Calculate SHA-256 hash of a file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "        return file_hash\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating hash for {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Read the labels file\n",
    "print(\"\\nReading labels file...\")\n",
    "df_labels = pd.read_csv(COMBINED_LABELS_PATH)\n",
    "print(f\"Loaded {len(df_labels)} entries from labels file\")\n",
    "\n",
    "# Get all image files\n",
    "print(\"\\nScanning for image files...\")\n",
    "all_image_files = []\n",
    "for ext in extensions:\n",
    "    files = glob.glob(os.path.join(COMBINED_IMAGES_PATH, f\"*{ext}\"))\n",
    "    all_image_files.extend(files)\n",
    "\n",
    "print(f\"Found {len(all_image_files)} image files\")\n",
    "\n",
    "# Compute hashes for all files\n",
    "print(\"\\nComputing file hashes to identify duplicates...\")\n",
    "file_hashes = {}\n",
    "hash_to_files = defaultdict(list)\n",
    "\n",
    "for img_path in tqdm(all_image_files):\n",
    "    file_hash = get_file_hash(img_path)\n",
    "    if file_hash:\n",
    "        file_hashes[img_path] = file_hash\n",
    "        hash_to_files[file_hash].append(img_path)\n",
    "\n",
    "# Find duplicates (files with the same hash)\n",
    "duplicates = {h: files for h, files in hash_to_files.items() if len(files) > 1}\n",
    "\n",
    "print(f\"\\nFound {len(duplicates)} sets of duplicate images\")\n",
    "\n",
    "if duplicates:\n",
    "    print(\"\\nRemoving duplicates while preserving one copy of each image...\")\n",
    "    \n",
    "    # Track files to be removed\n",
    "    files_to_remove = []\n",
    "    files_to_keep = []\n",
    "    id_mapping = {}  # Map from removed ID to kept ID\n",
    "    \n",
    "    # Process each set of duplicates\n",
    "    for hash_val, duplicate_files in tqdm(duplicates.items()):\n",
    "        # Sort files so that we keep original images (without \"_aug_\") if possible\n",
    "        sorted_files = sorted(duplicate_files, \n",
    "                              key=lambda x: \"_aug_\" in os.path.basename(x))\n",
    "        \n",
    "        # Keep the first file (preferentially an original, non-augmented image)\n",
    "        file_to_keep = sorted_files[0]\n",
    "        files_to_keep.append(file_to_keep)\n",
    "        \n",
    "        keep_id = os.path.splitext(os.path.basename(file_to_keep))[0]\n",
    "        \n",
    "        # Mark the rest for removal\n",
    "        for file_to_remove in sorted_files[1:]:\n",
    "            files_to_remove.append(file_to_remove)\n",
    "            \n",
    "            # Create ID mapping for label updates\n",
    "            remove_id = os.path.splitext(os.path.basename(file_to_remove))[0]\n",
    "            id_mapping[remove_id] = keep_id\n",
    "    \n",
    "    # Backup and remove duplicate files\n",
    "    print(f\"\\nBacking up and removing {len(files_to_remove)} duplicate files...\")\n",
    "    \n",
    "    for file_path in tqdm(files_to_remove):\n",
    "        filename = os.path.basename(file_path)\n",
    "        backup_path = os.path.join(BACKUP_DIR, filename)\n",
    "        \n",
    "        # Backup the file\n",
    "        shutil.copy2(file_path, backup_path)\n",
    "        \n",
    "        # Remove the file\n",
    "        os.remove(file_path)\n",
    "    \n",
    "    # Update the labels file\n",
    "    print(\"\\nUpdating labels file to reflect removed duplicates...\")\n",
    "    \n",
    "    # Map to track if we've already updated a label to point to a particular ID\n",
    "    updated_ids = set()\n",
    "    rows_to_drop = []\n",
    "    \n",
    "    # First pass: update IDs that need mapping\n",
    "    for i, row in tqdm(df_labels.iterrows(), total=len(df_labels)):\n",
    "        id_code = row['id_code']\n",
    "        \n",
    "        # If this ID was removed, update it to point to the kept ID\n",
    "        if id_code in id_mapping:\n",
    "            # If we haven't already updated a row to use this kept ID\n",
    "            if id_mapping[id_code] not in updated_ids:\n",
    "                df_labels.at[i, 'id_code'] = id_mapping[id_code]\n",
    "                updated_ids.add(id_mapping[id_code])\n",
    "            else:\n",
    "                # Mark this row for removal since we already have a row pointing to the kept ID\n",
    "                rows_to_drop.append(i)\n",
    "    \n",
    "    # Drop duplicate rows\n",
    "    if rows_to_drop:\n",
    "        print(f\"Removing {len(rows_to_drop)} redundant label entries...\")\n",
    "        df_labels = df_labels.drop(rows_to_drop)\n",
    "    \n",
    "    # Save the updated labels file\n",
    "    backup_labels_path = os.path.join(BACKUP_DIR, \"labels_backup.csv\")\n",
    "    shutil.copy2(COMBINED_LABELS_PATH, backup_labels_path)\n",
    "    \n",
    "    df_labels.to_csv(COMBINED_LABELS_PATH, index=False)\n",
    "    print(f\"Updated labels file saved to {COMBINED_LABELS_PATH}\")\n",
    "    \n",
    "    # Verify the results\n",
    "    print(\"\\nVerifying results...\")\n",
    "    print(f\"Original image count: {len(all_image_files)}\")\n",
    "    \n",
    "    # Count remaining images\n",
    "    remaining_images = []\n",
    "    for ext in extensions:\n",
    "        files = glob.glob(os.path.join(COMBINED_IMAGES_PATH, f\"*{ext}\"))\n",
    "        remaining_images.extend(files)\n",
    "    \n",
    "    print(f\"Remaining image count: {len(remaining_images)}\")\n",
    "    print(f\"Removed duplicate count: {len(files_to_remove)}\")\n",
    "    print(f\"Updated label count: {len(df_labels)}\")\n",
    "    \n",
    "    print(\"\\nDuplicate removal process completed successfully!\")\n",
    "    print(f\"Removed {len(files_to_remove)} duplicate images\")\n",
    "    print(f\"Updated labels file now contains {len(df_labels)} entries\")\n",
    "    print(f\"Backup of all removed files saved to {BACKUP_DIR}\")\n",
    "else:\n",
    "    print(\"No duplicates found. No changes were made.\")\n",
    "\n",
    "# Final verification to ensure all labels have corresponding images\n",
    "print(\"\\nPerforming final verification...\")\n",
    "missing_images = 0\n",
    "\n",
    "for id_code in tqdm(df_labels['id_code']):\n",
    "    found = False\n",
    "    for ext in extensions:\n",
    "        img_path = os.path.join(COMBINED_IMAGES_PATH, f\"{id_code}{ext}\")\n",
    "        if os.path.exists(img_path):\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        missing_images += 1\n",
    "\n",
    "if missing_images > 0:\n",
    "    print(f\"WARNING: Found {missing_images} label entries without corresponding images\")\n",
    "    print(\"You may need to run the data validation script again to clean these up\")\n",
    "else:\n",
    "    print(\"All label entries have corresponding images. Dataset is clean!\")\n",
    "\n",
    "print(\"\\nDuplicate removal process complete!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-06T17:32:35.577992Z"
    }
   },
   "id": "48c00b07a75aa796"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# APTOS Dataset Simple Excel Cleanup\n",
    "# Removes entries from Excel file that don't have corresponding images\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define paths\n",
    "COMBINED_IMAGES_PATH = r\"E:\\IRP_dataset_new\\APTOS_combined_images\"\n",
    "COMBINED_LABELS_PATH = r\"E:\\IRP_dataset_new\\APTOS_labels_combined.csv\"\n",
    "BACKUP_DIR = r\"E:\\IRP_dataset_new\\backups\"  # Directory for backups\n",
    "\n",
    "# Extensions to check\n",
    "extensions = ['.png', '.jpg', '.jpeg', '.tif', '.tiff']\n",
    "\n",
    "print(\"APTOS Dataset Simple Excel Cleanup\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create backup directory if it doesn't exist\n",
    "os.makedirs(BACKUP_DIR, exist_ok=True)\n",
    "\n",
    "# Backup the original labels file\n",
    "backup_filename = \"APTOS_labels_combined_backup.csv\"\n",
    "backup_path = os.path.join(BACKUP_DIR, backup_filename)\n",
    "shutil.copy2(COMBINED_LABELS_PATH, backup_path)\n",
    "print(f\"Created backup of labels file at {backup_path}\")\n",
    "\n",
    "# Read the labels file\n",
    "print(\"\\nReading labels file...\")\n",
    "df_labels = pd.read_csv(COMBINED_LABELS_PATH)\n",
    "original_count = len(df_labels)\n",
    "print(f\"Loaded {original_count} entries from labels file\")\n",
    "\n",
    "# Get all image filenames without extensions\n",
    "print(\"\\nCollecting all image filenames...\")\n",
    "image_ids = set()\n",
    "for ext in extensions:\n",
    "    pattern = os.path.join(COMBINED_IMAGES_PATH, f\"*{ext}\")\n",
    "    for img_path in glob.glob(pattern):\n",
    "        filename = os.path.basename(img_path)\n",
    "        image_id = os.path.splitext(filename)[0]  # Remove extension\n",
    "        image_ids.add(image_id)\n",
    "\n",
    "print(f\"Found {len(image_ids)} unique image IDs\")\n",
    "\n",
    "# Check each Excel entry and mark those with missing images\n",
    "print(\"\\nChecking each Excel entry for corresponding image...\")\n",
    "missing_image_mask = ~df_labels['id_code'].isin(image_ids)\n",
    "missing_entries = df_labels[missing_image_mask]\n",
    "\n",
    "print(f\"Found {len(missing_entries)} entries without corresponding images\")\n",
    "\n",
    "# Remove entries with missing images\n",
    "if len(missing_entries) > 0:\n",
    "    # Save the list of entries to be removed\n",
    "    missing_entries_path = os.path.join(BACKUP_DIR, \"removed_entries.csv\")\n",
    "    missing_entries.to_csv(missing_entries_path, index=False)\n",
    "    print(f\"Saved list of removed entries to {missing_entries_path}\")\n",
    "    \n",
    "    # Create updated DataFrame with entries removed\n",
    "    df_updated = df_labels[~missing_image_mask].copy()\n",
    "    \n",
    "    # Save the updated Excel file\n",
    "    df_updated.to_csv(COMBINED_LABELS_PATH, index=False)\n",
    "    print(f\"Updated labels file saved to {COMBINED_LABELS_PATH}\")\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nCleanup Results:\")\n",
    "    print(f\"Original entry count: {original_count}\")\n",
    "    print(f\"Entries removed: {len(missing_entries)}\")\n",
    "    print(f\"Remaining entries: {len(df_updated)}\")\n",
    "    \n",
    "    # Print class distribution before and after if 'diagnosis' column exists\n",
    "    if 'diagnosis' in df_labels.columns:\n",
    "        print(\"\\nClass distribution before cleanup:\")\n",
    "        print(df_labels['diagnosis'].value_counts().to_dict())\n",
    "        \n",
    "        print(\"\\nClass distribution after cleanup:\")\n",
    "        print(df_updated['diagnosis'].value_counts().to_dict())\n",
    "else:\n",
    "    print(\"No entries need to be removed. All Excel entries have corresponding images.\")\n",
    "\n",
    "print(\"\\nExcel cleanup complete!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-06T17:32:35.584868Z"
    }
   },
   "id": "a2ae066f94a5c693"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating backup at E:\\IRP_dataset_new\\APTOS_labels_combined_backup.csv\n",
      "Reading CSV file from E:\\IRP_dataset_new\\APTOS_labels_combined.csv\n",
      "Original file contains 10954 entries\n",
      "Found 244 duplicate entries\n",
      "These represent 122 unique ID codes that appear multiple times\n",
      "Removed 122 duplicate entries\n",
      "Cleaned file contains 10832 entries\n",
      "Saving cleaned file to E:\\IRP_dataset_new\\APTOS_labels_combined.csv\n",
      "Duplicate removal complete!\n",
      "The original file with duplicates is backed up at: E:\\IRP_dataset_new\\APTOS_labels_combined_backup.csv\n"
     ]
    }
   ],
   "source": [
    "# Simple script to remove duplicate entries from the CSV file\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "csv_path = r\"E:\\IRP_dataset_new\\APTOS_labels_combined.csv\"\n",
    "backup_path = r\"E:\\IRP_dataset_new\\APTOS_labels_combined_backup.csv\"\n",
    "\n",
    "# Create a backup of the original file\n",
    "print(f\"Creating backup at {backup_path}\")\n",
    "shutil.copy2(csv_path, backup_path)\n",
    "\n",
    "# Read the CSV file\n",
    "print(f\"Reading CSV file from {csv_path}\")\n",
    "df = pd.read_csv(csv_path)\n",
    "original_count = len(df)\n",
    "print(f\"Original file contains {original_count} entries\")\n",
    "\n",
    "# Identify duplicates\n",
    "duplicate_mask = df.duplicated(subset=['id_code'], keep=False)\n",
    "duplicate_entries = df[duplicate_mask]\n",
    "unique_duplicated_ids = duplicate_entries['id_code'].nunique()\n",
    "\n",
    "print(f\"Found {len(duplicate_entries)} duplicate entries\")\n",
    "print(f\"These represent {unique_duplicated_ids} unique ID codes that appear multiple times\")\n",
    "\n",
    "# Remove duplicates, keeping the first occurrence\n",
    "df_cleaned = df.drop_duplicates(subset=['id_code'], keep='first')\n",
    "removed_count = original_count - len(df_cleaned)\n",
    "\n",
    "print(f\"Removed {removed_count} duplicate entries\")\n",
    "print(f\"Cleaned file contains {len(df_cleaned)} entries\")\n",
    "\n",
    "# Save the cleaned dataframe back to the CSV file\n",
    "print(f\"Saving cleaned file to {csv_path}\")\n",
    "df_cleaned.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Duplicate removal complete!\")\n",
    "print(f\"The original file with duplicates is backed up at: {backup_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:36:17.728221300Z",
     "start_time": "2025-04-06T17:36:17.618333Z"
    }
   },
   "id": "9dcf72883675a939"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying dataset consistency...\n",
      "Excel file contains 10832 entries\n",
      "Number of unique ID codes in Excel: 10832\n",
      "Image folder contains 10832 files\n",
      "\n",
      "✓ SUCCESS: Excel file and image folder are perfectly synchronized!\n",
      "Both contain exactly 10832 items.\n"
     ]
    }
   ],
   "source": [
    "# Simple script to verify that Excel IDs match image folder contents\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def verify_dataset_consistency():\n",
    "    # Define file paths\n",
    "    excel_path = r\"E:\\IRP_dataset_new\\APTOS_labels_combined.csv\"\n",
    "    images_path = r\"E:\\IRP_dataset_new\\APTOS_combined_images\"\n",
    "    \n",
    "    # Extensions to check\n",
    "    extensions = ['.png', '.jpg', '.jpeg', '.tif', '.tiff']\n",
    "    \n",
    "    print(\"Verifying dataset consistency...\")\n",
    "    \n",
    "    # Read the Excel file\n",
    "    df = pd.read_csv(excel_path)\n",
    "    excel_count = len(df)\n",
    "    unique_ids_count = df['id_code'].nunique()\n",
    "    print(f\"Excel file contains {excel_count} entries\")\n",
    "    print(f\"Number of unique ID codes in Excel: {unique_ids_count}\")\n",
    "    \n",
    "    # Count image files\n",
    "    image_files = []\n",
    "    for ext in extensions:\n",
    "        files = glob.glob(os.path.join(images_path, f\"*{ext}\"))\n",
    "        image_files.extend(files)\n",
    "    \n",
    "    image_count = len(image_files)\n",
    "    print(f\"Image folder contains {image_count} files\")\n",
    "    \n",
    "    # Check for match\n",
    "    if excel_count == image_count and excel_count == unique_ids_count:\n",
    "        print(\"\\n✓ SUCCESS: Excel file and image folder are perfectly synchronized!\")\n",
    "        print(f\"Both contain exactly {excel_count} items.\")\n",
    "    else:\n",
    "        print(\"\\n⚠ WARNING: Excel file and image folder counts don't match.\")\n",
    "        print(f\"Excel entries: {excel_count}\")\n",
    "        print(f\"Unique ID codes: {unique_ids_count}\")\n",
    "        print(f\"Image files: {image_count}\")\n",
    "    \n",
    "    return {\n",
    "        'excel_count': excel_count,\n",
    "        'unique_ids_count': unique_ids_count,\n",
    "        'image_count': image_count,\n",
    "        'is_synchronized': excel_count == image_count and excel_count == unique_ids_count\n",
    "    }\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    verify_dataset_consistency()\n",
    "    \n",
    "#122 duplicates were found after augmenting"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:38:02.530633300Z",
     "start_time": "2025-04-06T17:38:02.059741300Z"
    }
   },
   "id": "8fc8e7a6db9ee606"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple ID Verification\n",
      "==============================\n",
      "Reading Excel file...\n",
      "Found 10832 IDs in Excel file\n",
      "Scanning image directory...\n",
      "Found 10832 images in directory\n",
      "\n",
      "Found 0 IDs in Excel without corresponding images\n",
      "\n",
      "Found 0 images without corresponding IDs in Excel\n",
      "\n",
      "Verification complete!\n"
     ]
    }
   ],
   "source": [
    "# Simple script to check if Excel IDs exist in image folder\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define paths\n",
    "COMBINED_IMAGES_PATH = r\"E:\\IRP_dataset_new\\APTOS_combined_images\"\n",
    "COMBINED_LABELS_PATH = r\"E:\\IRP_dataset_new\\APTOS_labels_combined.csv\"\n",
    "\n",
    "# Extensions to check\n",
    "extensions = ['.png', '.jpg', '.jpeg', '.tif', '.tiff']\n",
    "\n",
    "print(\"Simple ID Verification\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Read the Excel file\n",
    "print(\"Reading Excel file...\")\n",
    "df = pd.read_csv(COMBINED_LABELS_PATH)\n",
    "excel_ids = df['id_code'].tolist()\n",
    "print(f\"Found {len(excel_ids)} IDs in Excel file\")\n",
    "\n",
    "# Get all image files in the directory\n",
    "print(\"Scanning image directory...\")\n",
    "all_images = []\n",
    "for ext in extensions:\n",
    "    images = glob.glob(os.path.join(COMBINED_IMAGES_PATH, f\"*{ext}\"))\n",
    "    all_images.extend(images)\n",
    "\n",
    "# Extract IDs from image filenames\n",
    "image_ids = [os.path.splitext(os.path.basename(img))[0] for img in all_images]\n",
    "print(f\"Found {len(image_ids)} images in directory\")\n",
    "\n",
    "# Find IDs in Excel that don't have images\n",
    "missing_images = []\n",
    "for excel_id in excel_ids:\n",
    "    if excel_id not in image_ids:\n",
    "        missing_images.append(excel_id)\n",
    "\n",
    "print(f\"\\nFound {len(missing_images)} IDs in Excel without corresponding images\")\n",
    "if missing_images:\n",
    "    print(\"First 10 missing IDs:\")\n",
    "    for missing_id in missing_images[:10]:\n",
    "        print(f\"  {missing_id}\")\n",
    "    \n",
    "    # Option to save missing IDs to file\n",
    "    print(\"\\nDo you want to save the complete list of missing IDs to a file? (yes/no)\")\n",
    "    save_option = input().strip().lower()\n",
    "    if save_option == 'yes':\n",
    "        output_path = r\"E:\\IRP_dataset_new\\missing_ids.csv\"\n",
    "        pd.DataFrame({'id_code': missing_images}).to_csv(output_path, index=False)\n",
    "        print(f\"Saved missing IDs to {output_path}\")\n",
    "\n",
    "# Find images that don't have IDs in Excel\n",
    "extra_images = []\n",
    "for image_id in image_ids:\n",
    "    if image_id not in excel_ids:\n",
    "        extra_images.append(image_id)\n",
    "\n",
    "print(f\"\\nFound {len(extra_images)} images without corresponding IDs in Excel\")\n",
    "if extra_images:\n",
    "    print(\"First 10 extra images:\")\n",
    "    for extra_id in extra_images[:10]:\n",
    "        print(f\"  {extra_id}\")\n",
    "    \n",
    "    # Option to save extra image IDs to file\n",
    "    print(\"\\nDo you want to save the complete list of extra image IDs to a file? (yes/no)\")\n",
    "    save_option = input().strip().lower()\n",
    "    if save_option == 'yes':\n",
    "        output_path = r\"E:\\IRP_dataset_new\\extra_images.csv\"\n",
    "        pd.DataFrame({'id_code': extra_images}).to_csv(output_path, index=False)\n",
    "        print(f\"Saved extra image IDs to {output_path}\")\n",
    "\n",
    "print(\"\\nVerification complete!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-06T17:40:40.523411100Z",
     "start_time": "2025-04-06T17:40:37.153408700Z"
    }
   },
   "id": "84937067f42d417"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3b92a100e212deb4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
